import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split


df = pd.read_csv('../data/boston_housing.csv')


# 1. Handle Missing Values
print("Missing Values in Each Column:")
print(df.isnull().sum())


# # If there were missing values, we could impute them (e.g., with median), but this dataset has none.
# # For demonstration, let's assume we impute with median if any were present:
# for column in df.columns:
#     if df[column].isnull().sum() > 0:
#         df[column].fillna(df[column].median(), inplace=True)


# 2. Encode Categorical Variables
# 'chas' is binary (0 or 1), already suitable for modeling, no encoding needed.
# 'rad' is an index (1-24), could be treated as categorical or ordinal. We'll one-hot encode it.
df_encoded = pd.get_dummies(df, columns=['rad'], prefix='rad', drop_first=True)  # Avoid multicollinearity


# 3. Normalize/Standardize Numerical Features
# Separate features and target
X = df_encoded.drop('medv', axis=1)
y = df_encoded['medv']


# List of numerical columns (excluding 'chas' which is binary, and 'rad' which is now encoded)
numerical_cols = ['crim', 'zn', 'indus', 'nox', 'rm', 'age', 'dis', 'tax', 'ptratio', 'b', 'lstat']


# 4. Standardize features (mean=0, std=1)
scaler = StandardScaler()
X[numerical_cols] = scaler.fit_transform(X[numerical_cols])

print("\nFirst 5 rows of standardized features:")
print(X.head())


# 5. Split the Data into Training and Testing Sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("\nTraining set shape:", X_train.shape, y_train.shape)
print("Testing set shape:", X_test.shape, y_test.shape)


# Optional: Visualize distribution of a standardized feature
plt.figure(figsize=(8, 6))
sns.histplot(X['rm'], kde=True)
plt.title('Distribution of Standardized Average Rooms (rm)')
plt.xlabel('Standardized rm')
plt.show()



